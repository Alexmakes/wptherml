{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wptherml.wpml import multilayer\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import basinhopping\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "structure = {\n",
    "\n",
    "        'Material_List' : ['Air','J-Agg','TiO2', 'AlN','Ag', 'Air'],\n",
    "        ### Thicknesses just chosen arbitrarily, replace with \"optimal\" values\n",
    "        'Thickness_List': [0, 15e-9,8e-9, 8e-9, 300e-9, 0 ],\n",
    "        'Lambda_List': [300e-9, 1500e-9, 1000],\n",
    "        'Temperature': 300,\n",
    "        'Gradient_List':[2,3],\n",
    "        'LIGHTBULB': 1,\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "### create instance of multilayer structure\n",
    "cc = multilayer(structure)\n",
    "\n",
    "### how many degrees of freedom will we vary over?\n",
    "length = len(cc.gradient_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function *update_multilayer(x0)* that will take an array of thicknesses *x0* \n",
    "(in nm) for the layers-to-be-varied, update the multilayer with those new\n",
    "thicknesses, and return the enhancement factor. Also define a function\n",
    "*analytic_grad(x0)* that works the same way, but returns the gradient with \n",
    "respect to the layer thicknesses of the layers-to-be-varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr1 -5.869629358600855 [-1.23664410e-07 -7.51520907e-07]\n",
      "Gr2 -5.869629358600855 [-1.19757537e-07 -7.37331440e-07]\n",
      "Gr1 -1.194051024128294 [0.06334859 0.08808193]\n",
      "Gr2 -1.194051024128294 [0.06334859 0.08808194]\n"
     ]
    }
   ],
   "source": [
    "def update_multilayer(x0):\n",
    "    dim = len(x0)\n",
    "    ### update each layer-to-be-varied\n",
    "    for i in range(2,dim+2):\n",
    "        cc.d[i]= x0[i-2]*1e-9\n",
    "    ### recompute fresnel quantities\n",
    "    cc.fresnel()\n",
    "    ### recompute enhancement factor\n",
    "    cc.jagg_enhancement()\n",
    "    ### return negative of enhancement factor \n",
    "    ### recall that scipy's optimize functions will find minimum, \n",
    "    ### so we need to give them the negative of the objective\n",
    "    ### we wish to maximize\n",
    "    return -cc.jagg_enhancement_val\n",
    "\n",
    "def analytic_grad(x0):\n",
    "    dim = len(x0)\n",
    "    g = np.zeros(dim)\n",
    "    ### update multilayer and fresnel quantities\n",
    "    cur = update_multilayer(x0)\n",
    "    ### update gradient of fresnel quantities\n",
    "    cc.fresnel_prime()\n",
    "    ### compute gradient of objective\n",
    "    cc.jagg_enhancement_prime()\n",
    "    g = cc.jagg_enhancement_grad\n",
    "    \n",
    "    return -g*1e-9\n",
    "\n",
    "def numeric_grad(x0):\n",
    "    dim = len(x0)\n",
    "    h0 = 0.01*np.ones(dim)\n",
    "    g = np.zeros(dim)\n",
    "    for i in range(0,dim):\n",
    "        xpass = np.copy(x0)\n",
    "        fx = x0[i] + h0[i]\n",
    "        bx = x0[i] - h0[i]\n",
    "        xpass[i] = fx\n",
    "        efx = update_multilayer(xpass)\n",
    "        xpass[i] = bx\n",
    "        ebx = update_multilayer(xpass)\n",
    "        run = 2*h0[i]\n",
    "        g[i] = (efx-ebx)/run\n",
    "    return g\n",
    "\n",
    "### function that calls both update_multilayer and analytic_gradient\n",
    "### and returns both objective and gradient\n",
    "def SuperFunc1(x0):\n",
    "    en = update_multilayer(x0)\n",
    "    gr = analytic_grad(x0)\n",
    "    return en, gr\n",
    "\n",
    "### function that calls both update_multilayer and numerical_gradient\n",
    "### and returns both objective and gradient\n",
    "def SuperFunc(x0):\n",
    "    en = update_multilayer(x0)\n",
    "    gr = numeric_grad(x0)\n",
    "    return en, gr\n",
    "\n",
    "### Just to confirm that both \n",
    "### analytic and numerical gradients give the\n",
    "### same results to within acceptable precision\n",
    "x0 = np.zeros(2)\n",
    "### this is geometry that maximizes enhancement, \n",
    "### so gradient should nearly vanish\n",
    "x0[0] = 7.152392\n",
    "x0[1] = 21.5937\n",
    "en1, gr1 = SuperFunc1(x0)\n",
    "en2, gr2 = SuperFunc(x0)\n",
    "print(\"Gr1\",en1,gr1)\n",
    "print(\"Gr2\",en2,gr2)\n",
    "\n",
    "### this is a random point, gradient probably won't vanish\n",
    "x0[0] = 13.1\n",
    "x0[1] = 42.1\n",
    "en1, gr1 = SuperFunc1(x0)\n",
    "en2, gr2 = SuperFunc(x0)\n",
    "print(\"Gr1\",en1,gr1)\n",
    "print(\"Gr2\",en2,gr2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will customize a few things related to the Basin Hopping algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs is  [20. 20.]\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15319069 21.59313562] -5.869629358072668 1 1567090054.548938\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.1517196  21.59421735] -5.869629358741984 1 1567090064.668501\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15208577 21.59396741] -5.8696293586767 1 1567090077.755581\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.14719064 21.59751495] -5.869629352432368 1 1567090093.271789\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15151103 21.59438367] -5.8696293587094015 1 1567090108.900678\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15179791 21.59417346] -5.8696293587227695 1 1567090124.913762\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15320191 21.59298079] -5.869629357464769 1 1567090136.475067\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15118837 21.59459775] -5.869629358662446 1 1567090156.4611259\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15170574 21.59421882] -5.869629358747982 1 1567090169.872027\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15122963 21.594597  ] -5.869629358631633 1 1567090184.3584378\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15170533 21.59421461] -5.869629358749438 1 1567090196.6047208\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15148382 21.59442667] -5.8696293586487585 1 1567090207.728148\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15206266 21.59390809] -5.869629358656316 1 1567090221.9814851\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.1544328  21.59237138] -5.869629355922627 1 1567090236.185456\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15144301 21.59445147] -5.869629358656871 1 1567090250.8936808\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15142666 21.59451756] -5.869629358416626 1 1567090264.5200262\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15173298 21.5941762 ] -5.8696293587432855 1 1567090277.804679\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15191098 21.59406556] -5.869629358736806 1 1567090287.130124\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15223882 21.59382193] -5.86962935866297 1 1567090301.7013838\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.13595323 21.60571428] -5.869629282053018 1 1567090313.399308\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15023622 21.59518952] -5.869629357867257 1 1567090326.412407\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.14721371 21.59737039] -5.86962935225148 1 1567090338.236894\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.1515707  21.59437389] -5.869629358623725 1 1567090351.342952\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15167655 21.59424751] -5.8696293587426736 1 1567090363.374242\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.1520276  21.59400392] -5.869629358698609 1 1567090378.3930829\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15186951 21.59414908] -5.869629358649792 1 1567090392.528637\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.1506796  21.59507688] -5.869629358001679 1 1567090403.9156291\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15157969 21.59430263] -5.869629358744645 1 1567090417.744545\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15143954 21.59445528] -5.869629358652564 1 1567090432.69869\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15749998 21.58998478] -5.869629348394328 1 1567090442.33642\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15118981 21.5946128 ] -5.869629358644873 1 1567090454.631766\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15172017 21.59435882] -5.86962935803817 1 1567090464.440786\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.1520656 21.5939864] -5.869629358672782 1 1567090479.1377149\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15207885 21.59397166] -5.869629358679691 1 1567090491.0711381\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15369792 21.59275829] -5.869629357528175 1 1567090505.279496\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15198473 21.59399566] -5.869629358721551 1 1567090523.481458\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15148061 21.59437906] -5.869629358733058 1 1567090534.8103008\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15183558 21.59411549] -5.869629358744767 1 1567090545.089461\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15299517 21.59329368] -5.869629358230611 1 1567090558.6085582\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15233894 21.59374009] -5.869629358623487 1 1567090568.311163\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15363667 21.59290317] -5.869629357361559 1 1567090582.9219358\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15161371 21.59430571] -5.869629358724602 1 1567090595.837557\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15341858 21.59292538] -5.869629357794928 1 1567090606.3000169\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15021037 21.59536796] -5.869629357907693 1 1567090619.9501572\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15898603 21.58880738] -5.86962934198653 1 1567090634.817503\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15069383 21.59491243] -5.86962935840295 1 1567090644.723734\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15114329 21.59449623] -5.869629358225837 1 1567090657.658238\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15794036 21.5896549 ] -5.8696293467444285 1 1567090671.255455\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.14641334 21.59811815] -5.869629349969196 1 1567090681.00211\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15189478 21.5940879 ] -5.869629358733455 1 1567090692.9595149\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15304602 21.59331965] -5.8696293580140715 1 1567090703.4639082\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15174484 21.59421244] -5.869629358723977 1 1567090716.032993\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15179911 21.5941258 ] -5.869629358738698 1 1567090731.7757509\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15183828 21.5940867 ] -5.869629358722262 1 1567090745.5780952\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15273299 21.59349448] -5.869629358403311 1 1567090756.497279\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15144667 21.59444239] -5.869629358674551 1 1567090767.62333\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15162224 21.59427019] -5.869629358747439 1 1567090779.012274\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15163408 21.59428531] -5.869629358733742 1 1567090793.34875\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15750932 21.59013758] -5.869629347860957 1 1567090801.807671\n",
      "!!!!!!!!!!!!!!  at minimum: [ 7.15045083 21.59523842] -5.869629357869994 1 1567090813.823219\n",
      "[ 7.15170533 21.59421461]\n",
      "-5.869629358749438\n"
     ]
    }
   ],
   "source": [
    "### This function will be called by the BH algorithm \n",
    "### each time a new local optimum is found... will\n",
    "### print geometry, objective, and absolute time\n",
    "def print_fun(x, f, accepted):\n",
    "    c_time = time.time()\n",
    "    ### note: print x[0]... x[L]\n",
    "    print(\"!!!!!!!!!!!!!!  at minimum:\",x, f, int(accepted),c_time)\n",
    "\n",
    "### This defines how new initial configurations are chosen by the BH\n",
    "### for each sub-optimization\n",
    "def my_take_step(x):\n",
    "    xnew = np.copy(x)\n",
    "    dim = len(xnew)\n",
    "    ### pick random numbers for position\n",
    "    for i in range(0,dim):\n",
    "        rn = 30*np.abs(np.random.randn())\n",
    "        xnew[i] = rn\n",
    "    return xnew\n",
    "\n",
    "### specifically defines bounds for Basin Hopping\n",
    "### algorithm... if a sub-optimization lands outside of\n",
    "### these bounds and converges, the solution will not be accepted\n",
    "class MyBounds(object):\n",
    "      ### note xmax and xmin need to have as many elements as there are thicknesses that are varied\n",
    "    def __init__(self, xmax=35*np.ones(length), xmin=np.ones(length)):\n",
    "        self.xmax = np.array(xmax)\n",
    "        self.xmin = np.array(xmin)\n",
    "    def __call__(self, **kwargs):\n",
    "        x = kwargs[\"x_new\"]\n",
    "        tmax = bool(np.all(x <= self.xmax))\n",
    "        tmin = bool(np.all(x >= self.xmin))\n",
    "        return tmax and tmin\n",
    "\n",
    "### the bounds for L-BFGS-B updates!  If an update \n",
    "### within a L-BFGS updates takes you outside these bounds,\n",
    "### the update step will be modified to prevent going out of bounds\n",
    "bfgs_xmin = np.ones(length)\n",
    "bfgs_xmax = 35*np.ones(length)\n",
    "\n",
    "# rewrite the bounds in the way required by L-BFGS-B\n",
    "bfgs_bounds = [(low, high) for low, high in zip(bfgs_xmin, bfgs_xmax)]\n",
    "\n",
    "minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"jac\": True, \"bounds\": bfgs_bounds}\n",
    "mybounds = MyBounds()\n",
    "\n",
    "### note: initialize xs to be length L where L are the number of layers to be varied\n",
    "xs = np.ones(length)*20\n",
    "print(\"xs is \",xs)\n",
    "\n",
    "ret = basinhopping(SuperFunc, xs, minimizer_kwargs=minimizer_kwargs, niter=60, take_step=my_take_step, callback=print_fun, accept_test=mybounds)\n",
    "\n",
    "print(ret.x)\n",
    "print(update_multilayer(ret.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
